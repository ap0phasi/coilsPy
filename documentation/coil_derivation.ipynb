{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coils: Modelling dynamical systems as self-dependent flows of probability\n",
    "\n",
    "As our aim is to describe system dynamics as flows of probability, the most natural starting place is with probability currents found in quantum mechanics, which gives the continuity equation for probability:\n",
    "$$\n",
    "\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot \\mathbf{j} = 0\n",
    "$$\n",
    "\n",
    "Where $\\rho$ is the probability density and $\\mathbf{j}$ is the probability current. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unidirectional and one-dimensional flows of probability, the probability current continuity equation can be discretized as:\n",
    "$$\n",
    "\\frac{\\partial \\rho_{x}}{\\partial t} = \\frac{J_{x-\\Delta x} - J_{x}}{\\Delta x}\n",
    "$$\n",
    "\n",
    "to express the change of the probability density within some segment of space $x$ of width $\\Delta x$, where $J_{x-\\Delta x}$ is the probability flow in the segment \"upstream\" of $x$ and $J_{x}$ is the flow in $x$, effectively describing the flow into and out of $x$, respectively. \n",
    "\n",
    "Further discretizing in time gives us\n",
    "\n",
    "$$\n",
    "\\frac{\\rho_{x}(t + \\Delta t) - \\rho_{x}(t)}{\\Delta t} =  \\frac{J_{x-\\Delta x} - J_{x}}{\\Delta x}\n",
    "$$\n",
    "\n",
    "which can be rearranged as:\n",
    "\n",
    "$$\n",
    "\\rho_{x}(t + \\Delta t)\\Delta x - \\rho_{x}(t)\\Delta x = J_{x-\\Delta x}\\Delta t - J_{x}\\Delta t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rho_{x}(t + \\Delta t)\\Delta x$ is equivalent the probability in that section $P_{x}(t+\\Delta t)$, so it follows that\n",
    "\n",
    "\n",
    "$$\n",
    "P_{x}(t+\\Delta t) = J_{x-\\Delta x}\\Delta t - J_{x}\\Delta t + P_{x}(t) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the flows of probability in this manner, we can naturally make the association\n",
    "\n",
    "$$\n",
    "\\left[P_{x}(t+\\Delta t) \\;\\middle|\\; P_{x - \\Delta x}(t)\\right]P_{x - \\Delta x}(t) = J_{x-\\Delta x}\\Delta t\n",
    "$$\n",
    "\n",
    "as $J_{x-\\Delta x}\\Delta t$ describes the probability that is contributed to $x$ by the \"upstream\" segment $x-\\Delta x$. Therefore the use of conditional probabilities to describe the probability of being in $x$ at $t+\\Delta t$ given being in $x-\\Delta x$ at $t$ is analogous to probability flow. Conditional probabilties can be used for each of our flow terms, giving us the equation \n",
    "\n",
    "$$\n",
    "P_{x}(t+\\Delta t) = \\left[P_{x}(t+\\Delta t) \\;\\middle|\\; P_{x - \\Delta x}(t)\\right]P_{x - \\Delta x}(t) - \\left[P_{x+\\Delta x}(t+\\Delta t)\\;\\middle|\\;P_{x}(t)\\right]P_{x}(t) + P_{x}(t) \n",
    "$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$\n",
    "P_{x}(t+\\Delta t) = \\left[P_{x}(t+\\Delta t) \\;\\middle|\\; P_{x - \\Delta x}(t)\\right]P_{x - \\Delta x}(t) + (1 - \\left[P_{x+\\Delta x}(t+\\Delta t)\\;\\middle|\\;P_{x}(t)\\right])P_{x}(t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are dealing only in the case of unidirectional flows in this example, so probability cannot flow from $x$ to $x-\\Delta x$. Instead, probability in $x$ can only either stay in $x$ or flow to $x + \\Delta x$. Given these two sole possibilites, we know\n",
    "$$\n",
    "\\left[P_{x+\\Delta x}(t+\\Delta t) \\;\\middle|\\; P_{x}(t) \\right] + \\left[P_{x}(t+\\Delta t) \\;\\middle|\\; P_{x}(t) \\right] = 1\n",
    "$$\n",
    "\n",
    "Which can be substituted into our equation to get\n",
    "\n",
    "$$\n",
    "P_{x}(t+\\Delta t) = \\left[P_{x}(t+\\Delta t) \\;\\middle|\\; P_{x - \\Delta x}(t)\\right]P_{x - \\Delta x}(t) + \\left[P_{x}(t+\\Delta t) \\;\\middle|\\; P_{x}(t) \\right]P_{x}(t)\n",
    "$$\n",
    "\n",
    "Expressed in this manner, an obvious pattern is apparent where $P_{x}(t+\\Delta t)$ is simply the sum of everything contributing to it from time $t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we relax our orignal constraints of unidirectionality, one-dimensionality, and even locality (allowing probability to flow from spaces spatially separated from one another, and in fact removing notions of space altogether), we get the general expression:\n",
    "\n",
    "$$\n",
    "P_{i}(t+\\Delta t) = \\sum\\limits_{j=1}^{N}\\left[P_{i}(t+\\Delta t) \\;\\middle|\\; P_{j}(t)\\right]P_{j}(t) \\Rightarrow \\sum\\limits_{i=1}^{N}\\left[P_{i}(t+\\Delta t) \\;\\middle|\\; P_{j}(t)\\right] = 1 \\quad \\forall j \\quad\\&\\quad \\sum\\limits_{i=1}^{N}P_{i}(t) = 1 \\quad\\forall t\n",
    "$$\n",
    "\n",
    "The criteria $\\sum\\limits_{i=1}^{N}\\left[P_{i}(t+\\Delta t) \\;\\middle|\\; P_{j}(t)\\right] = 1 \\quad \\forall j$ is just the statement that probability in $j$ must go *somewhere*, inclusive of staying in $j$. While it is obvious that $\\sum\\limits_{i=1}^{N}P_{i}(t) = 1 \\quad\\forall t$, as we are describing probability dynamics, this condition becomes important as we further extend our formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also express our equation in vector notation:\n",
    "$$\n",
    "\\mathbf{P}(t + \\Delta t) = \\mathbf{A}(t + \\Delta t)\\mathbf{P}(t) \\Rightarrow \\sum\\limits_{i=1}^{N}\\mathbf{A}_{ij}(t + \\Delta t) = 1 \\quad \\forall j \\quad\\&\\quad \\sum\\mathbf{P}(t) = 1 \\quad\\forall t\n",
    "$$\n",
    "\n",
    "If we assume $A$ is constant, we find that we have essentially arrived at the general Markov equation. This comes as no surpise, of course, as Markov Chains are one of the most common ways of describing flows of probability. However, in our case we have no reason to believe that $\\mathbf{A}$ is not changing in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "When seeking to describe the temporal dynamics of $\\mathbf{A}$, we find that because $\\mathbf{A}$ must normalize across $i$ for all $j$, each \"column\" $\\mathbf{A}_{\\cdot j}(t + \\Delta t)$ can be treated similar to $\\mathbf{P}(t)$ where\n",
    "$$\n",
    "\\mathbf{A}_{\\cdot j}(t + \\Delta t) = \\mathbf{M}_{\\cdot j \\cdot}\\mathbf{x} \\Rightarrow \\sum\\limits_{i=1}^{N}\\mathbf{M}_{ijk}(t + \\Delta t) = 1 \\quad \\forall k \\quad\\&\\quad \\sum \\mathbf{x} = 1\n",
    "$$\n",
    "\n",
    "Or more generally\n",
    "\n",
    "$$\n",
    "\\mathbf{A}(t + \\Delta t) = \\mathbf{M}\\mathbf{x} \\iff \\sum\\limits_{i=1}^{N}\\mathbf{M}_{ijk} = 1 \\quad \\forall jk \\quad\\&\\quad \\sum \\mathbf{x} = 1\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{x}$ is a vector of the same length as $\\mathbf{P}$, and is normalized, summing to 1. \n",
    "\n",
    "Note that we can continue this process for all normalized parts of $\\mathbf{M}$ and so on *ad infinitum*, but for our purposes we can stop at assuming $\\mathbf{M}$ does not change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\mathbf{x}$ must be a normalized vector of the same length as $\\mathbf{P}$, we realize $\\mathbf{A}(t)$ contains a collection of suitable candidates in the form of its columns. Alternatively, $\\mathbf{P}(t)$ is also a viable candidate for $\\mathbf{x}$, meaning that we can select $\\mathbf{x}$ as:\n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{A}_{\\cdot s}(t) \\iff s \\in \\{1, 2, \\ldots, N\\} \\quad or \\quad \\mathbf{x} = \\mathbf{P}(t)\n",
    "$$\n",
    "\n",
    "That is to say, picking it from $\\mathbf{A}(t)$'s columns or as $\\mathbf{P}(t)$. This operation enriches our formulation to describe system dynamics as self-dependent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the discontinuity of simply picking the value of $\\mathbf{x}$ from $\\mathbf{A}(t)$'s columns or as $\\mathbf{P}(t)$ may pose challenges for practical applications, the selection of $\\mathbf{x}$ can take a continuous form while maintaining conservation by expressing $\\mathbf{x}$ as a weighted sum across $\\mathbf{A(t)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\sum_{s=1}^{N} w_s \\mathbf{A}_{\\cdot s}(t) \\iff \\sum_{s=1}^{N} w_{i} = 1\n",
    "$$\n",
    "\n",
    "However as $\\mathbf{P(t)}$ is also a worthy candidate for $\\mathbf{x}$, a more complete expression would be:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\sum_{s=1}^{N+1} w_s [\\mathbf{A}(t),\\mathbf{P}(t)]_{\\cdot s} \\iff \\sum_{s=1}^{N+1} w_{i} = 1\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{A}(t)$ and $\\mathbf{P}(t)$ are combined to form a matrix with $N+1$ columns, and the weighting vector $w$ (now of length $N+1$) is applied across them. \n",
    "\n",
    "We will explore how $w$ can be calculated in a later exercise, where in keeping with the theme of interconnectedness and self-dependence, $w$ will be a function of $\\mathbf{A}$ and $\\mathbf{P}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting our selection mechanism into the above equations, we are left with a full coil equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}(t + \\Delta t) = \\mathbf{M}\\sum_{s=1}^{N+1} w_s \\left[\\mathbf{A}(t),\\mathbf{P}(t)\\right]_{\\cdot s}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{P}(t + \\Delta t) = \\mathbf{A}(t + \\Delta t)\\mathbf{P}(t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Demonstration\n",
    "\n",
    "We can easily test coils using PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coil Test 1: Updating State with Transition Tensor\n",
    "The first test will be simply demonstrate how for some transition tensor $\\mathbf{A}$ where $\\sum\\limits_{i=1}^{N}\\mathbf{A}_{ij}$, $\\mathbf{A}$ can be used to describe the flow of probability and therefore update our probability state tensor $\\mathbf{P}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish initial transition tensor and state tensor\n",
    "states = 4\n",
    "\n",
    "initial_transition_tensor = torch.softmax(torch.randn(states, states), dim = 0)\n",
    "initial_state_tensor = torch.softmax(torch.randn(states), dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch's softmax activation ensures that our tensors normalize correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_transition_tensor.sum(dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see that the probability state tensor updates while maintaining conservation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original State Tensor:  tensor([0.4101, 0.1566, 0.2904, 0.1429])\n",
      "New State Tensor:  tensor([0.1833, 0.5368, 0.1131, 0.1668])\n",
      "State Tensor Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate simple normalization\n",
    "state_tensor = torch.matmul(initial_transition_tensor, initial_state_tensor)\n",
    "print(\"Original State Tensor: \",initial_state_tensor)\n",
    "print(\"New State Tensor: \",state_tensor)\n",
    "print(\"State Tensor Sum:\",sum(state_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coil Test 2: Simple Interaction with Discrete Selection\n",
    "\n",
    "Similarly, we can create an interaction tensor $\\mathbf{M}$ where $\\sum\\limits_{i=1}^{N}\\mathbf{M}_{ijk} = 1$, which can be used to update our transition tensor, given some normalized tensor $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_tensor = torch.softmax(torch.randn(states, states, states), dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will just select $\\mathbf{x}$ as one of the columns of $\\mathbf{A}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State Tensor:  tensor([0.2968, 0.2598, 0.1913, 0.2521])\n",
      "State Tensor Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "selection = 1\n",
    "transition_tensor = torch.mul(interaction_tensor, initial_transition_tensor[:,selection].unsqueeze(0)).sum(-1)\n",
    "state_tensor = torch.matmul(transition_tensor, initial_state_tensor)\n",
    "print(\"New State Tensor: \",state_tensor)\n",
    "print(\"State Tensor Sum:\",sum(state_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coil Test 3: Simple Interaction with Weighted Selection\n",
    "We can also perform selection with weightings, where first we will assemble our tensor $\\left[\\mathbf{A}(t),\\mathbf{P}(t)\\right]$, which contains all worthy candidates for $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_subgroups = torch.cat((initial_state_tensor.unsqueeze(-1), initial_transition_tensor), dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can simply apply a weighting across this tensor to determine $\\mathbf{x}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State Tensor:  tensor([0.3091, 0.2437, 0.1911, 0.2561])\n",
      "State Tensor Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "w = torch.softmax(torch.randn(states + 1), dim = 0)\n",
    "x = torch.matmul(norm_subgroups,w)\n",
    "transition_tensor = torch.mul(interaction_tensor, x.unsqueeze(0)).sum(-1)\n",
    "state_tensor = torch.matmul(transition_tensor, initial_state_tensor)\n",
    "print(\"New State Tensor: \",state_tensor)\n",
    "print(\"State Tensor Sum:\",sum(state_tensor))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate that this weighting procedure is the more generalized form of selection, we can force our weights to be zero except for a single selection point, emulating the discrete selection found in Coil Test 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State Tensor:  tensor([0.2968, 0.2598, 0.1913, 0.2521])\n",
      "State Tensor Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# For exact selection\n",
    "w_single_select = torch.zeros(states + 1)\n",
    "w_single_select[selection + 1] = 1 # We increment up 1 because norm_subgroups' first element is the state tensor\n",
    "x = torch.matmul(norm_subgroups, w_single_select)\n",
    "transition_tensor = torch.mul(interaction_tensor, x.unsqueeze(0)).sum(-1)\n",
    "state_tensor = torch.matmul(transition_tensor, initial_state_tensor)\n",
    "print(\"New State Tensor: \",state_tensor)\n",
    "print(\"State Tensor Sum:\",sum(state_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which gives the same answer as if we were to discretely select $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Formulation\n",
    "\n",
    "We can extend the complexity of coils further by relaxing the assumption that every normalized column in $\\left[\\mathbf{A}(t),\\mathbf{P}(t)\\right]$ shares the same $\\mathbf{M}$. Instead, there can be a unique $\\mathbf{M}$ for each column. This updates the coil equations into the more general form\n",
    "$$\n",
    "\\mathbf{A}(t + \\Delta t) = \\sum_{s=1}^{N+1} w_s \\left(\\mathbf{M}_{s}\\left[\\mathbf{A}(t),\\mathbf{P}(t)\\right]_{\\cdot s}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{P}(t + \\Delta t) = \\mathbf{A}(t + \\Delta t)\\mathbf{P}(t)\n",
    "$$\n",
    "\n",
    "where $\\left(\\mathbf{M}_s\\left[\\mathbf{A}(t),\\mathbf{P}(t)\\right]_{\\cdot s}\\right)$ for each $s$ generates a candidate transition matrix, which is weighted by $w$ to get the new transition matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coil Test 4: True Coil with Unique Interactions\n",
    "\n",
    "Because, in this coil formulation, each normalized column in $\\left[\\mathbf{A}(t),\\mathbf{P}(t)\\right]$ as a unique $\\mathbf{M}$, we will generate $N+1$ interaction tensors $\\mathbf{M}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_tensors = torch.softmax(torch.randn(states, states, states, states+1), dim = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A unique $\\mathbf{M}$ multiplied by its respective normalized group effectively gives a candidate transition tensor $\\mathbf{A}(t+\\Delta t)$. So we can therefore apply out weightings across these candidates to get our transition tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State Tensor:  tensor([0.2307, 0.2629, 0.1756, 0.3308])\n",
      "State Tensor Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "transition_tensor = torch.zeros_like(transition_tensor)\n",
    "for s in range(states+1):\n",
    "    transition_tensor = transition_tensor + w[s] * torch.matmul(interaction_tensors[:,:,:,s], norm_subgroups[:,s])\n",
    "    \n",
    "state_tensor = torch.matmul(transition_tensor, initial_state_tensor)\n",
    "print(\"New State Tensor: \",state_tensor)\n",
    "print(\"State Tensor Sum:\",sum(state_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can demonstrate that this method of using unique interaction tensors is a generalization of the previous method of a single shared interaction tensor by forcing each of the interaction tensors to be the original interaction tensor, which is equivalent to Coil Test 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New State Tensor:  tensor([0.3091, 0.2437, 0.1911, 0.2561])\n",
      "State Tensor Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# To check equivalence to previous method\n",
    "for s in range(states+1):\n",
    "    interaction_tensors[:,:,:,s] = interaction_tensor\n",
    "    \n",
    "transition_tensor = torch.zeros_like(transition_tensor)\n",
    "for s in range(states+1):\n",
    "    transition_tensor = transition_tensor + w[s] * torch.matmul(interaction_tensors[:,:,:,s], norm_subgroups[:,s])\n",
    "    \n",
    "state_tensor = torch.matmul(transition_tensor, initial_state_tensor)\n",
    "print(\"New State Tensor: \",state_tensor)\n",
    "print(\"State Tensor Sum:\",sum(state_tensor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coilspy-1sgZ1XBf-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
