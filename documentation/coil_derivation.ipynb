{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coils: Modelling dynamical systems as self-dependent flows of probability\n",
    "\n",
    "As our aim is to describe system dynamics as flows of probability, the most natural starting place is with probability currents found in quantum mechanics, which gives the continuity equation for probability:\n",
    "$$\n",
    "\\frac{\\partial \\rho}{\\partial t} + \\nabla \\cdot \\mathbf{j} = 0\n",
    "$$\n",
    "\n",
    "Where $\\rho$ is the probability density and $\\mathbf{j}$ is the probability current. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unidirectional and one-dimensional flows of probability, the probability current continuity equation can be discretized as:\n",
    "$$\n",
    "\\frac{\\partial \\rho_{x}}{\\partial t} = \\frac{J_{x-\\Delta x} - J_{x}}{\\Delta x}\n",
    "$$\n",
    "\n",
    "to express the change of the probability density within some segment of space $x$ of width $\\Delta x$, where $J_{x-\\Delta x}$ is the probability flow in the segment \"upstream\" of $x$ and $J_{x}$ is the flow in $x$, effectively describing the flow into and out of $x$, respectively. \n",
    "\n",
    "Further discretizing in time gives us\n",
    "\n",
    "$$\n",
    "\\frac{\\rho_{x}(t + \\Delta t) - \\rho_{x}(t)}{\\Delta t} =  \\frac{J_{x-\\Delta x} - J_{x}}{\\Delta x}\n",
    "$$\n",
    "\n",
    "which can be rearranged as:\n",
    "\n",
    "$$\n",
    "\\rho_{x}(t + \\Delta t)\\Delta x - \\rho_{x}(t)\\Delta x = J_{x-\\Delta x}\\Delta t - J_{x}\\Delta t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rho_{x}(t + \\Delta t)\\Delta x$ is equivalent the probability in that section $P_{x}(t+\\Delta t)$, so it follows that\n",
    "\n",
    "\n",
    "$$\n",
    "P_{x}(t+\\Delta t) = J_{x-\\Delta x}\\Delta t - J_{x}\\Delta t + P_{x}(t) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the flows of probability in this manner, we can naturally make the association\n",
    "\n",
    "$$\n",
    "\\left[P_{x}(t+\\Delta t) \\;\\middle|\\; P_{x - \\Delta x}(t)\\right]P_{x - \\Delta x}(t) = J_{x-\\Delta x}\\Delta t\n",
    "$$\n",
    "\n",
    "as $J_{x-\\Delta x}\\Delta t$ describes the probability that is contributed to $x$ by the \"upstream\" segment $x-\\Delta x$. Therefore the use of conditional probabilities to describe the probability of being in $x$ at $t+\\Delta t$ given being in $x-\\Delta x$ at $t$ is analogous to probability flow. Conditional probabilties can be used for each of our flow terms, giving us the equation \n",
    "\n",
    "$$\n",
    "P_{x}(t+\\Delta t) = \\left[P_{x}(t+\\Delta t) \\;\\middle|\\; P_{x - \\Delta x}(t)\\right]P_{x - \\Delta x}(t) - \\left[P_{x+\\Delta x}(t+\\Delta t)\\;\\middle|\\;P_{x}(t)\\right]P_{x}(t) + P_{x}(t) \n",
    "$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$\n",
    "P_{x}(t+\\Delta t) = \\left[P_{x}(t+\\Delta t) \\;\\middle|\\; P_{x - \\Delta x}(t)\\right]P_{x - \\Delta x}(t) + (1 - \\left[P_{x+\\Delta x}(t+\\Delta t)\\;\\middle|\\;P_{x}(t)\\right])P_{x}(t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we are dealing only in the case of unidirectional flows in this example, so probability cannot flow from $x$ to $x-\\Delta x$. Instead, probability in $x$ can only either stay in $x$ or flow to $x + \\Delta x$. Given these two sole possibilites, we know\n",
    "$$\n",
    "\\left[P_{x+\\Delta x}(t+\\Delta t) \\;\\middle|\\; P_{x}(t) \\right] + \\left[P_{x}(t+\\Delta t) \\;\\middle|\\; P_{x}(t) \\right] = 1\n",
    "$$\n",
    "\n",
    "Which can be substituted into our equation to get\n",
    "\n",
    "$$\n",
    "P_{x}(t+\\Delta t) = \\left[P_{x}(t+\\Delta t) \\;\\middle|\\; P_{x - \\Delta x}(t)\\right]P_{x - \\Delta x}(t) + \\left[P_{x}(t+\\Delta t) \\;\\middle|\\; P_{x}(t) \\right]P_{x}(t)\n",
    "$$\n",
    "\n",
    "Expressed in this manner, an obvious pattern is apparent where $P_{x}(t+\\Delta t)$ is simply the sum of everything contributing to it from time $t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we relax our orignal constraints of unidirectionality, one-dimensionality, and even locality (allowing probability to flow from spaces spatially separated from one another, and in fact removing notions of space altogether), we get the general expression:\n",
    "\n",
    "$$\n",
    "P_{i}(t+\\Delta t) = \\sum\\limits_{j=1}^{N}\\left[P_{i}(t+\\Delta t) \\;\\middle|\\; P_{j}(t)\\right]P_{j}(t) \\Rightarrow \\sum\\limits_{i=1}^{N}\\left[P_{i}(t+\\Delta t) \\;\\middle|\\; P_{j}(t)\\right] = 1 \\quad \\forall j \\quad\\&\\quad \\sum\\limits_{i=1}^{N}P_{i}(t) = 1 \\quad\\forall t\n",
    "$$\n",
    "\n",
    "The criteria $\\sum\\limits_{i=1}^{N}\\left[P_{i}(t+\\Delta t) \\;\\middle|\\; P_{j}(t)\\right] = 1 \\quad \\forall j$ is just the statement that probability in $j$ must go *somewhere*, inclusive of staying in $j$. While it is obvious that $\\sum\\limits_{i=1}^{N}P_{i}(t) = 1 \\quad\\forall t$, as we are describing probability dynamics, this condition becomes important as we further extend our formulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also express our equation in vector notation:\n",
    "$$\n",
    "\\mathbf{P}(t + \\Delta t) = \\mathbf{A}(t + \\Delta t)\\mathbf{P}(t) \\Rightarrow \\sum\\limits_{i=1}^{N}\\mathbf{A}_{ij}(t + \\Delta t) = 1 \\quad \\forall j \\quad\\&\\quad \\sum\\mathbf{P}(t) = 1 \\quad\\forall t\n",
    "$$\n",
    "\n",
    "If we assume $A$ is constant, we find that we have essentially arrived at the general Markov equation. This comes as no surpise, of course, as Markov Chains are one of the most common ways of describing flows of probability. However, in our case we have no reason to believe that $\\mathbf{A}$ is not changing in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "When seeking to describe the temporal dynamics of $\\mathbf{A}$, we find that because $\\mathbf{A}$ must normalize across $i$ for all $j$, each \"column\" $\\mathbf{A}_{\\cdot j}(t + \\Delta t)$ can be treated similar to $\\mathbf{P}(t)$ where\n",
    "$$\n",
    "\\mathbf{A}_{\\cdot j}(t + \\Delta t) = \\mathbf{M}_{\\cdot j \\cdot}\\mathbf{x} \\Rightarrow \\sum\\limits_{i=1}^{N}\\mathbf{M}_{ijk}(t + \\Delta t) = 1 \\quad \\forall k \\quad\\&\\quad \\sum \\mathbf{x} = 1\n",
    "$$\n",
    "\n",
    "Or more generally\n",
    "\n",
    "$$\n",
    "\\mathbf{A}(t + \\Delta t) = \\mathbf{M}\\mathbf{x} \\iff \\sum\\limits_{i=1}^{N}\\mathbf{M}_{ijk} = 1 \\quad \\forall jk \\quad\\&\\quad \\sum \\mathbf{x} = 1\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{x}$ is a vector of the same length as $\\mathbf{P}$, and is normalized, summing to 1. \n",
    "\n",
    "Note that we can continue this process for all normalized parts of $\\mathbf{M}$ and so on *ad infinitum*, but for our purposes we can stop at assuming $\\mathbf{M}$ does not change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As $\\mathbf{x}$ must be a normalized vector of the same length as $\\mathbf{P}$, we realize $\\mathbf{A}(t)$ contains a collection of suitable candidates in the form of its columns. Alternatively, $\\mathbf{P}(t)$ is also a viable candidate for $\\mathbf{x}$, meaning that we can select $\\mathbf{x}$ as:\n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{A}_{\\cdot s}(t) \\iff s \\in \\{1, 2, \\ldots, N\\} \\quad or \\quad \\mathbf{x} = \\mathbf{P}(t)\n",
    "$$\n",
    "\n",
    "That is to say, picking it from $\\mathbf{A}(t)$'s columns or as $\\mathbf{P}(t)$. This operation enriches our formulation to describe system dynamics as self-dependent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the discontinuity of simply picking the value of $\\mathbf{x}$ from $\\mathbf{A}(t)$'s columns or as $\\mathbf{P}(t)$ may pose challenges for practical applications, the selection of $\\mathbf{x}$ can take a continuous form while maintaining conservation by expressing $\\mathbf{x}$ as a weighted sum across $\\mathbf{A(t)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\sum_{s=1}^{N} w_s \\mathbf{A}_{\\cdot s}(t) \\iff \\sum_{s=1}^{N} w_{i} = 1\n",
    "$$\n",
    "\n",
    "However as $\\mathbf{P(t)}$ is also a worthy candidate for $\\mathbf{x}$, a more complete expression would be:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\sum_{s=1}^{N+1} w_s [\\mathbf{A}(t),\\mathbf{P}(t)]_{\\cdot s} \\iff \\sum_{s=1}^{N+1} w_{i} = 1\n",
    "$$\n",
    "\n",
    "Where $\\mathbf{A}(t)$ and $\\mathbf{P}(t)$ are combined to form a matrix with $N+1$ columns, and the weighting vector $w$ (now of length $N+1$) is applied across them. \n",
    "\n",
    "We will explore how $w$ can be calculated in a later exercise, where in keeping with the theme of interconnectedness and self-dependence, $w$ will be a function of $\\mathbf{A}$ and $\\mathbf{P}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting our selection mechanism into the above equations, we are left with a full coil equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}(t + \\Delta t) = \\mathbf{M}\\sum_{s=1}^{N+1} w_s \\left[\\mathbf{A}(t),\\mathbf{P}(t)\\right]_{\\cdot s}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{P}(t + \\Delta t) = \\mathbf{A}(t + \\Delta t)\\mathbf{P}(t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish initial transition tensor and state tensor\n",
    "states = 4\n",
    "\n",
    "initial_transition_tensor = torch.softmax(torch.randn(states, states), dim = 0)\n",
    "initial_state_tensor = torch.softmax(torch.randn(states), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate simple normalization\n",
    "state_tensor = torch.matmul(initial_transition_tensor, initial_state_tensor)\n",
    "sum(state_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_tensor = torch.softmax(torch.randn(states, states, states), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2583, 0.2470, 0.2248, 0.2699])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selection = 1\n",
    "transition_tensor = torch.mul(interaction_tensor, initial_transition_tensor[:,selection].unsqueeze(0)).sum(-1)\n",
    "state_tensor = torch.matmul(transition_tensor, initial_state_tensor)\n",
    "print(state_tensor)\n",
    "sum(state_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_subgroups = torch.cat((initial_state_tensor.unsqueeze(-1), initial_transition_tensor), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2258, 0.2344, 0.3222, 0.2175])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.softmax(torch.randn(states + 1), dim = 0)\n",
    "x = torch.matmul(norm_subgroups,w)\n",
    "transition_tensor = torch.mul(interaction_tensor, x.unsqueeze(0)).sum(-1)\n",
    "state_tensor = torch.matmul(transition_tensor, initial_state_tensor)\n",
    "print(state_tensor)\n",
    "sum(state_tensor)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2583, 0.2470, 0.2248, 0.2699])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For exact selection\n",
    "w = torch.zeros(states + 1)\n",
    "w[selection + 1] = 1\n",
    "x = torch.matmul(norm_subgroups,w)\n",
    "transition_tensor = torch.mul(interaction_tensor, x.unsqueeze(0)).sum(-1)\n",
    "state_tensor = torch.matmul(transition_tensor, initial_state_tensor)\n",
    "print(state_tensor)\n",
    "sum(state_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Formulation\n",
    "\n",
    "We can extend the complexity of coils further by relaxing the assumption that every normalized column in $\\left[\\mathbf{A}(t),\\mathbf{P}(t)\\right]$ shares the same $\\mathbf{M}$. Instead, there can be a unique $\\mathbf{M}$ for each column. This updates the coil equations into the more general form\n",
    "$$\n",
    "\\mathbf{A}(t + \\Delta t) = \\sum_{s=1}^{N+1} w_s \\left(\\mathbf{M}_s\\left[\\mathbf{A}(t),\\mathbf{P}(t)\\right]_{\\cdot s}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{P}(t + \\Delta t) = \\mathbf{A}(t + \\Delta t)\\mathbf{P}(t)\n",
    "$$\n",
    "\n",
    "where $\\left(\\mathbf{M}_s\\left[\\mathbf{A}(t),\\mathbf{P}(t)\\right]_{\\cdot s}\\right)$ for each $s$ generates a candidate transition matrix, which is weighted by $w$ to get the new transition matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_tensors = torch.softmax(torch.randn(states, states, states, states+1), dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2309, 0.3262, 0.1377, 0.3052])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_tensor = torch.zeros_like(transition_tensor)\n",
    "for s in range(states+1):\n",
    "    transition_tensor = transition_tensor + w[s] * torch.matmul(interaction_tensors[:,:,:,s], norm_subgroups[:,s])\n",
    "    \n",
    "state_tensor = torch.matmul(transition_tensor, initial_state_tensor)\n",
    "print(state_tensor)\n",
    "sum(state_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2583, 0.2470, 0.2248, 0.2699])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check equivalence to previous method\n",
    "for s in range(states+1):\n",
    "    interaction_tensors[:,:,:,s] = interaction_tensor\n",
    "    \n",
    "transition_tensor = torch.zeros_like(transition_tensor)\n",
    "for s in range(states+1):\n",
    "    transition_tensor = transition_tensor + w[s] * torch.matmul(interaction_tensors[:,:,:,s], norm_subgroups[:,s])\n",
    "    \n",
    "state_tensor = torch.matmul(transition_tensor, initial_state_tensor)\n",
    "print(state_tensor)\n",
    "sum(state_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coilspy-1sgZ1XBf-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
