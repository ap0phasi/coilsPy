{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        # Initialize the interaction tensor as a learnable parameter\n",
    "        self.interaction_tensor = nn.Parameter(torch.randn(n_features, n_features, n_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be of size [batch, length, n_features]\n",
    "        batch, length, n_features = x.size()\n",
    "\n",
    "        # Process each [n_features] vector across batch and length\n",
    "        output = x.new_empty(batch, length, n_features)\n",
    "        for b in range(batch):\n",
    "            for l in range(length):\n",
    "                state_tensor = x[b, l, :]  # Shape: [n_features]\n",
    "                # Step 1: Multiply state tensor by interaction tensor to get transition tensor\n",
    "                # We manually implement the multiplication to match your operation\n",
    "                transition_tensor = torch.einsum('i,ijk->jk', state_tensor, self.interaction_tensor)\n",
    "                # Step 2: Multiply the transition tensor by the state tensor\n",
    "                # Resulting shape: [n_features]\n",
    "                output[b, l, :] = torch.matmul(transition_tensor, state_tensor)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLayerVectorized(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(CustomLayerVectorized, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        # Initialize the interaction tensor as a learnable parameter\n",
    "        self.interaction_tensor = nn.Parameter(torch.randn(n_features, n_features, n_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be of size [batch, length, n_features]\n",
    "        batch, length, n_features = x.size()\n",
    "\n",
    "        # Pre-allocate output tensor\n",
    "        output = x.new_empty(batch, length, n_features)\n",
    "\n",
    "        # Loop over length, but vectorize over the batch\n",
    "        for l in range(length):\n",
    "            # Extract all vectors at position l across all batches\n",
    "            state_tensor = x[:, l, :]  # Shape: [batch, n_features]\n",
    "            \n",
    "            # Vectorized operation for all batches\n",
    "            # Step 1: Calculate the transition tensor\n",
    "            # Since we cannot directly use einsum for batched operation in this specific scenario,\n",
    "            # we manually broadcast and multiply to achieve the intended result.\n",
    "            # This involves expanding dimensions to enable broadcasting.\n",
    "            state_tensor_expanded = state_tensor.unsqueeze(1).expand(-1, n_features, -1)  # Shape: [batch, n_features, n_features]\n",
    "            interaction_tensor_expanded = self.interaction_tensor.unsqueeze(0).expand(batch, -1, -1, -1)  # Shape: [batch, n_features, n_features, n_features]\n",
    "            # Multiply and sum over the last dimension to get the transition tensor\n",
    "            transition_tensor = torch.einsum('bik,bijk->bij', state_tensor_expanded, interaction_tensor_expanded)\n",
    "            \n",
    "            # Step 2: Multiply the transition tensor by the state tensor to get the output\n",
    "            output[:, l, :] = torch.einsum('bij,bj->bi', transition_tensor, state_tensor)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = 16\n",
    "# batch, length, dim = 2, 64, n_features\n",
    "# x = torch.randn(batch, length, dim)\n",
    "# model = CustomLayerVectorized(\n",
    "#     n_features = n_features\n",
    "# )\n",
    "# y = model(x)\n",
    "\n",
    "# print(x.shape)\n",
    "# assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0882, 0.0566, 0.4550, 0.0727, 0.1311, 0.1963],\n",
      "        [0.2407, 0.1104, 0.2277, 0.2421, 0.1738, 0.0053],\n",
      "        [0.1226, 0.0789, 0.1218, 0.1194, 0.2616, 0.2957],\n",
      "        [0.2379, 0.0828, 0.2108, 0.0702, 0.2090, 0.1894],\n",
      "        [0.0027, 0.3202, 0.2005, 0.2144, 0.0303, 0.2318]])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def torch_randnorm(size, dim=0):\n",
    "    # Generate a random tensor\n",
    "    rand_tensor = torch.rand(size)\n",
    "    \n",
    "    # Normalize along the specified dimension\n",
    "    sum_along_dim = torch.sum(rand_tensor, dim=dim, keepdim=True)\n",
    "    normalized_tensor = rand_tensor / sum_along_dim\n",
    "    \n",
    "    return normalized_tensor\n",
    "\n",
    "# Example usage\n",
    "normalized_tensor = torch_randnorm([5,6], dim=1)\n",
    "print(normalized_tensor)\n",
    "print(normalized_tensor.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionModule(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(InteractionModule, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        # Initialize a set of interaction tensors, one for the state tensor and one for each column of the transition tensor\n",
    "        self.interaction_tensors = nn.ParameterList([nn.Parameter(torch_randnorm([n_features, n_features, n_features], dim = 1)) for _ in range(n_features + 1)])\n",
    "\n",
    "    def forward(self, state_tensor, previous_transition_tensor):\n",
    "        # Get batch size\n",
    "        batch = state_tensor.shape[0]\n",
    "        # Assuming previous_transition_tensors is a list of transition tensors from the previous step\n",
    "        candidates = []\n",
    "        for i in range(self.n_features + 1):\n",
    "            if i == 0:  # Interaction with the state tensor\n",
    "                current_tensor = state_tensor\n",
    "            else:  # Interaction with columns of the previous transition tensor\n",
    "                current_tensor = previous_transition_tensor[:, :, i - 1]\n",
    "\n",
    "            interaction_tensor = self.interaction_tensors[i]\n",
    "            current_tensor_expanded = current_tensor.unsqueeze(1).expand(-1, self.n_features, -1)  # Shape: [batch, n_features, n_features]\n",
    "            interaction_tensor_expanded = interaction_tensor.unsqueeze(0).expand(batch, -1, -1, -1)  # Shape: [batch, n_features, n_features, n_features]\n",
    "            # Multiply and sum over the last dimension to get the transition tensor\n",
    "            candidate = torch.einsum('bik,bijk->bij', current_tensor_expanded, interaction_tensor_expanded)\n",
    "            \n",
    "            candidates.append(candidate)\n",
    "            \n",
    "        candidates_tensor = torch.stack(candidates, dim = -1)\n",
    "        return candidates_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayerExtended(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(CustomLayerExtended, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.interaction_module = InteractionModule(n_features)\n",
    "        self.selector_module = SelectorModule(n_features + 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, length, n_features = x.size()\n",
    "        output = x.new_empty(batch, length, n_features)\n",
    "\n",
    "        # Initialize previous transition tensors (for the first step)\n",
    "        # Assuming it's a list of zero tensors for simplicity\n",
    "        previous_transition_tensor = torch.zeros(batch, n_features, n_features)\n",
    "\n",
    "        for l in range(length):\n",
    "            state_tensor = x[:, l, :]\n",
    "            # Generate candidates\n",
    "            candidates = self.interaction_module(state_tensor, previous_transition_tensor)\n",
    "            # Select one candidate\n",
    "            selected_transition_tensor = self.selector_module(candidates)\n",
    "            # Update the previous_transition_tensors for the next iteration\n",
    "            previous_transition_tensor = selected_transition_tensor\n",
    "            # Compute output for this step\n",
    "            output[:, l, :] = torch.matmul(selected_transition_tensor, state_tensor.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        return output, selected_transition_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelectorModule(nn.Module):\n",
    "    def __init__(self, num_slices):\n",
    "        super(SelectorModule, self).__init__()\n",
    "        # A simple linear layer to compute importance scores for each slice\n",
    "        self.importance = nn.Linear(num_slices, num_slices)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is of shape [batch_size, height, width, num_slices]\n",
    "        batch_size, _, _, num_slices = x.shape\n",
    "\n",
    "        # Compute importance scores by reducing x across spatial dimensions\n",
    "        # Here, we take the mean of x across the spatial dimensions to get a vector per slice\n",
    "        x_reduced = x.mean(dim=[1, 2])  # Shape: [batch_size, num_slices]\n",
    "\n",
    "        # Compute scores for each slice\n",
    "        scores = self.importance(x_reduced)  # Shape: [batch_size, num_slices]\n",
    "\n",
    "        # Apply softmax to get weights for each slice\n",
    "        weights = F.softmax(scores, dim=-1)  # Shape: [batch_size, num_slices]\n",
    "\n",
    "        # Correct application of weights:\n",
    "        # We need to ensure weights are applied across the num_slices dimension correctly.\n",
    "        # Since weights are [batch_size, num_slices] and x is [batch_size, height, width, num_slices],\n",
    "        # we permute x to bring num_slices to the front for broadcasting.\n",
    "        x_permuted = x.permute(0, 3, 1, 2)  # Shape: [batch_size, num_slices, height, width]\n",
    "        \n",
    "        # Now, multiply by weights. We need to reshape weights to [batch_size, num_slices, 1, 1] for broadcasting.\n",
    "        weighted_slices = x_permuted * weights.view(batch_size, num_slices, 1, 1)\n",
    "        \n",
    "        # Finally, sum the weighted slices across the num_slices dimension (now the first dimension after permute)\n",
    "        selected = weighted_slices.sum(dim=1)  # Shape: [batch_size, height, width]\n",
    "\n",
    "        return selected\n",
    "\n",
    "# Example usage\n",
    "batch_size = 10\n",
    "tensor = torch.rand(batch_size, 5, 5, 6)  # Example tensor\n",
    "model = SelectorModule(num_slices=6)\n",
    "\n",
    "result = model(tensor)\n",
    "print(result.shape)  # Should print torch.Size([10, 5, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class EnhancedModelWithMHA(nn.Module):\n",
    "    def __init__(self, n_features, num_heads):\n",
    "        super(EnhancedModelWithMHA, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(n_features)\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=n_features, num_heads=num_heads)\n",
    "        self.feed_forward = nn.Linear(n_features, n_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(1, 0, 2)  # Adjust for MHA: [seq_len, batch, features]\n",
    "        x = self.pos_encoder(x)\n",
    "        attn_output, _ = self.multihead_attn(x, x, x)\n",
    "        output = F.relu(self.feed_forward(attn_output))\n",
    "        output = output.permute(1, 0, 2)  # Adjust back: [batch, seq_len, features]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def generate_normalized_multivariate_time_series(n_features, total_length, amplitude=1.0):\n",
    "    t = np.linspace(0, 100 * np.pi, total_length)\n",
    "    series = np.zeros((total_length, n_features))\n",
    "    for i in range(n_features):\n",
    "        series[:, i] = amplitude * np.cos(t * (i + 1) / n_features) + 10\n",
    "    \n",
    "    # Normalize such that each timestep's values sum to 1\n",
    "    series_sum = np.sum(series, axis=1, keepdims=True)\n",
    "    series_normalized = series / series_sum\n",
    "    \n",
    "    return series_normalized\n",
    "\n",
    "def segment_time_series(series, length):\n",
    "    # Assuming series is a numpy array of shape [total_length, n_features]\n",
    "    total_length, n_features = series.shape\n",
    "    segments = []\n",
    "    for start in range(0, total_length - length, length):\n",
    "        segment = series[start:start + length]\n",
    "        segments.append(segment)\n",
    "    return np.stack(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 4\n",
    "length = 640  # Segment length\n",
    "total_length = 10240  # Arbitrary total length for the generated series\n",
    "\n",
    "# Generate and segment the time series\n",
    "series = generate_normalized_multivariate_time_series(n_features, total_length)\n",
    "series_x = series[:-1,]\n",
    "series_y = series[1:,]\n",
    "\n",
    "segments_x = segment_time_series(series_x, length)\n",
    "segments_y = segment_time_series(series_y, length)\n",
    "\n",
    "# Convert to tensors\n",
    "segments_tensor_x = torch.tensor(segments_x, dtype=torch.float)\n",
    "segments_tensor_y = torch.tensor(segments_y, dtype=torch.float)\n",
    "\n",
    "# Prepare inputs and targets\n",
    "X = segments_tensor_x\n",
    "# Shift segments to the right by one timestep to create the targets\n",
    "Y =  segments_tensor_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.03248867020010948\n",
      "Epoch [20/100], Loss: 0.03218349441885948\n",
      "Epoch [30/100], Loss: 0.03173460811376572\n",
      "Epoch [40/100], Loss: 0.03148769214749336\n",
      "Epoch [50/100], Loss: 0.031460363417863846\n",
      "Epoch [60/100], Loss: 0.03146583214402199\n",
      "Epoch [70/100], Loss: 0.03146149963140488\n",
      "Epoch [80/100], Loss: 0.031457945704460144\n",
      "Epoch [90/100], Loss: 0.03145666792988777\n",
      "Epoch [100/100], Loss: 0.03145628795027733\n"
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Model\n",
    "num_heads = 4\n",
    "model = EnhancedModelWithMHA(n_features=n_features, num_heads=num_heads)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "\n",
    "    # Backward and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2302, 0.2306],\n",
       "        [0.2306, 0.2309],\n",
       "        [0.2309, 0.2312],\n",
       "        ...,\n",
       "        [0.2722, 0.2720],\n",
       "        [0.2720, 0.2719],\n",
       "        [0.2719, 0.2717]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([X[1,:,1],Y[1,:,1]], dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "mode": "lines",
         "name": "Model Output",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639
         ],
         "y": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0
         ]
        },
        {
         "mode": "lines",
         "name": "Target",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41,
          42,
          43,
          44,
          45,
          46,
          47,
          48,
          49,
          50,
          51,
          52,
          53,
          54,
          55,
          56,
          57,
          58,
          59,
          60,
          61,
          62,
          63,
          64,
          65,
          66,
          67,
          68,
          69,
          70,
          71,
          72,
          73,
          74,
          75,
          76,
          77,
          78,
          79,
          80,
          81,
          82,
          83,
          84,
          85,
          86,
          87,
          88,
          89,
          90,
          91,
          92,
          93,
          94,
          95,
          96,
          97,
          98,
          99,
          100,
          101,
          102,
          103,
          104,
          105,
          106,
          107,
          108,
          109,
          110,
          111,
          112,
          113,
          114,
          115,
          116,
          117,
          118,
          119,
          120,
          121,
          122,
          123,
          124,
          125,
          126,
          127,
          128,
          129,
          130,
          131,
          132,
          133,
          134,
          135,
          136,
          137,
          138,
          139,
          140,
          141,
          142,
          143,
          144,
          145,
          146,
          147,
          148,
          149,
          150,
          151,
          152,
          153,
          154,
          155,
          156,
          157,
          158,
          159,
          160,
          161,
          162,
          163,
          164,
          165,
          166,
          167,
          168,
          169,
          170,
          171,
          172,
          173,
          174,
          175,
          176,
          177,
          178,
          179,
          180,
          181,
          182,
          183,
          184,
          185,
          186,
          187,
          188,
          189,
          190,
          191,
          192,
          193,
          194,
          195,
          196,
          197,
          198,
          199,
          200,
          201,
          202,
          203,
          204,
          205,
          206,
          207,
          208,
          209,
          210,
          211,
          212,
          213,
          214,
          215,
          216,
          217,
          218,
          219,
          220,
          221,
          222,
          223,
          224,
          225,
          226,
          227,
          228,
          229,
          230,
          231,
          232,
          233,
          234,
          235,
          236,
          237,
          238,
          239,
          240,
          241,
          242,
          243,
          244,
          245,
          246,
          247,
          248,
          249,
          250,
          251,
          252,
          253,
          254,
          255,
          256,
          257,
          258,
          259,
          260,
          261,
          262,
          263,
          264,
          265,
          266,
          267,
          268,
          269,
          270,
          271,
          272,
          273,
          274,
          275,
          276,
          277,
          278,
          279,
          280,
          281,
          282,
          283,
          284,
          285,
          286,
          287,
          288,
          289,
          290,
          291,
          292,
          293,
          294,
          295,
          296,
          297,
          298,
          299,
          300,
          301,
          302,
          303,
          304,
          305,
          306,
          307,
          308,
          309,
          310,
          311,
          312,
          313,
          314,
          315,
          316,
          317,
          318,
          319,
          320,
          321,
          322,
          323,
          324,
          325,
          326,
          327,
          328,
          329,
          330,
          331,
          332,
          333,
          334,
          335,
          336,
          337,
          338,
          339,
          340,
          341,
          342,
          343,
          344,
          345,
          346,
          347,
          348,
          349,
          350,
          351,
          352,
          353,
          354,
          355,
          356,
          357,
          358,
          359,
          360,
          361,
          362,
          363,
          364,
          365,
          366,
          367,
          368,
          369,
          370,
          371,
          372,
          373,
          374,
          375,
          376,
          377,
          378,
          379,
          380,
          381,
          382,
          383,
          384,
          385,
          386,
          387,
          388,
          389,
          390,
          391,
          392,
          393,
          394,
          395,
          396,
          397,
          398,
          399,
          400,
          401,
          402,
          403,
          404,
          405,
          406,
          407,
          408,
          409,
          410,
          411,
          412,
          413,
          414,
          415,
          416,
          417,
          418,
          419,
          420,
          421,
          422,
          423,
          424,
          425,
          426,
          427,
          428,
          429,
          430,
          431,
          432,
          433,
          434,
          435,
          436,
          437,
          438,
          439,
          440,
          441,
          442,
          443,
          444,
          445,
          446,
          447,
          448,
          449,
          450,
          451,
          452,
          453,
          454,
          455,
          456,
          457,
          458,
          459,
          460,
          461,
          462,
          463,
          464,
          465,
          466,
          467,
          468,
          469,
          470,
          471,
          472,
          473,
          474,
          475,
          476,
          477,
          478,
          479,
          480,
          481,
          482,
          483,
          484,
          485,
          486,
          487,
          488,
          489,
          490,
          491,
          492,
          493,
          494,
          495,
          496,
          497,
          498,
          499,
          500,
          501,
          502,
          503,
          504,
          505,
          506,
          507,
          508,
          509,
          510,
          511,
          512,
          513,
          514,
          515,
          516,
          517,
          518,
          519,
          520,
          521,
          522,
          523,
          524,
          525,
          526,
          527,
          528,
          529,
          530,
          531,
          532,
          533,
          534,
          535,
          536,
          537,
          538,
          539,
          540,
          541,
          542,
          543,
          544,
          545,
          546,
          547,
          548,
          549,
          550,
          551,
          552,
          553,
          554,
          555,
          556,
          557,
          558,
          559,
          560,
          561,
          562,
          563,
          564,
          565,
          566,
          567,
          568,
          569,
          570,
          571,
          572,
          573,
          574,
          575,
          576,
          577,
          578,
          579,
          580,
          581,
          582,
          583,
          584,
          585,
          586,
          587,
          588,
          589,
          590,
          591,
          592,
          593,
          594,
          595,
          596,
          597,
          598,
          599,
          600,
          601,
          602,
          603,
          604,
          605,
          606,
          607,
          608,
          609,
          610,
          611,
          612,
          613,
          614,
          615,
          616,
          617,
          618,
          619,
          620,
          621,
          622,
          623,
          624,
          625,
          626,
          627,
          628,
          629,
          630,
          631,
          632,
          633,
          634,
          635,
          636,
          637,
          638,
          639
         ],
         "y": [
          0.25305256247520447,
          0.25238439440727234,
          0.25171637535095215,
          0.25104910135269165,
          0.2503831684589386,
          0.24971921741962433,
          0.2490578442811966,
          0.24839965999126434,
          0.24774529039859772,
          0.24709533154964447,
          0.24645039439201355,
          0.2458111047744751,
          0.24517802894115448,
          0.24455179274082184,
          0.24393297731876373,
          0.24332216382026672,
          0.24271993339061737,
          0.24212686717510223,
          0.24154351651668549,
          0.2409704476594925,
          0.24040821194648743,
          0.23985734581947327,
          0.2393183559179306,
          0.2387917935848236,
          0.23827813565731049,
          0.23777788877487183,
          0.23729154467582703,
          0.2368195503950119,
          0.23636238276958466,
          0.2359204739332199,
          0.23549425601959229,
          0.23508411645889282,
          0.23469048738479614,
          0.2343137115240097,
          0.23395419120788574,
          0.23361223936080933,
          0.23328819870948792,
          0.23298236727714539,
          0.232695072889328,
          0.23242653906345367,
          0.23217706382274628,
          0.23194687068462372,
          0.2317361682653427,
          0.23154516518115997,
          0.23137402534484863,
          0.23122291266918182,
          0.23109197616577148,
          0.23098132014274597,
          0.23089104890823364,
          0.23082122206687927,
          0.23077191412448883,
          0.2307431548833847,
          0.2307349443435669,
          0.2307472974061966,
          0.23078018426895142,
          0.23083354532718658,
          0.23090733587741852,
          0.23100143671035767,
          0.23111575841903687,
          0.23125016689300537,
          0.23140452802181244,
          0.23157866299152374,
          0.23177237808704376,
          0.23198549449443817,
          0.23221777379512787,
          0.23246896266937256,
          0.23273882269859314,
          0.23302708566188812,
          0.23333342373371124,
          0.23365753889083862,
          0.2339991182088852,
          0.2343578189611435,
          0.23473326861858368,
          0.2351250946521759,
          0.23553290963172913,
          0.2359563261270523,
          0.23639492690563202,
          0.23684826493263245,
          0.23731592297554016,
          0.23779743909835815,
          0.23829233646392822,
          0.23880016803741455,
          0.23932042717933655,
          0.239852637052536,
          0.24039627611637115,
          0.2409508377313614,
          0.24151581525802612,
          0.24209065735340118,
          0.24267485737800598,
          0.24326784908771515,
          0.2438691109418869,
          0.24447806179523468,
          0.2450941801071167,
          0.2457168698310852,
          0.24634559452533722,
          0.24697977304458618,
          0.24761883914470673,
          0.2482622116804123,
          0.2489093393087387,
          0.24955964088439941,
          0.25021252036094666,
          0.25086745619773865,
          0.25152379274368286,
          0.2521810531616211,
          0.2528385818004608,
          0.25349584221839905,
          0.2541522979736328,
          0.25480735301971436,
          0.2554604411125183,
          0.2561110258102417,
          0.25675854086875916,
          0.2574024498462677,
          0.25804218649864197,
          0.25867724418640137,
          0.25930705666542053,
          0.25993114709854126,
          0.2605489492416382,
          0.2611599862575531,
          0.26176372170448303,
          0.2623596489429474,
          0.26294732093811035,
          0.26352623105049133,
          0.26409590244293213,
          0.2646558880805969,
          0.2652057111263275,
          0.2657449245452881,
          0.2662730813026428,
          0.2667897939682007,
          0.26729458570480347,
          0.2677870988845825,
          0.2682669162750244,
          0.2687336504459381,
          0.26918691396713257,
          0.26962634921073914,
          0.27005162835121155,
          0.27046236395835876,
          0.2708582580089569,
          0.2712389826774597,
          0.2716042101383209,
          0.27195367217063904,
          0.2722870409488678,
          0.2726041078567505,
          0.2729046046733856,
          0.2731882333755493,
          0.27345481514930725,
          0.2737041115760803,
          0.27393588423728943,
          0.274150013923645,
          0.274346262216568,
          0.2745244801044464,
          0.27468451857566833,
          0.2748262286186218,
          0.2749495208263397,
          0.2750542461872101,
          0.27514031529426575,
          0.27520766854286194,
          0.2752562165260315,
          0.27528589963912964,
          0.275296688079834,
          0.27528858184814453,
          0.2752615511417389,
          0.27521562576293945,
          0.27515077590942383,
          0.2750670611858368,
          0.2749645411968231,
          0.2748432755470276,
          0.27470335364341736,
          0.2745448648929596,
          0.27436789870262146,
          0.2741726040840149,
          0.27395913004875183,
          0.2737275958061218,
          0.2734781801700592,
          0.2732110917568207,
          0.27292653918266296,
          0.2726247012615204,
          0.27230581641197205,
          0.27197012305259705,
          0.27161791920661926,
          0.2712494432926178,
          0.27086499333381653,
          0.27046486735343933,
          0.27004939317703247,
          0.2696188688278198,
          0.26917368173599243,
          0.26871418952941895,
          0.26824072003364563,
          0.2677536904811859,
          0.26725348830223083,
          0.2667405307292938,
          0.2662152051925659,
          0.2656780183315277,
          0.26512935757637024,
          0.2645696997642517,
          0.2639995217323303,
          0.2634193003177643,
          0.2628295421600342,
          0.26223069429397583,
          0.2616233229637146,
          0.2610079050064087,
          0.26038500666618347,
          0.25975513458251953,
          0.25911885499954224,
          0.25847673416137695,
          0.2578292787075043,
          0.25717708468437195,
          0.25652074813842773,
          0.2558607757091522,
          0.25519779324531555,
          0.2545323967933655,
          0.25386515259742737,
          0.2531966269016266,
          0.2525274455547333,
          0.2518581748008728,
          0.2511894404888153,
          0.25052177906036377,
          0.24985584616661072,
          0.24919217824935913,
          0.24853140115737915,
          0.24787408113479614,
          0.24722078442573547,
          0.2465721219778061,
          0.24592863023281097,
          0.24529089033603668,
          0.24465946853160858,
          0.24403488636016846,
          0.24341772496700287,
          0.2428084909915924,
          0.24220770597457886,
          0.2416159063577652,
          0.24103358387947083,
          0.24046123027801514,
          0.23989932239055634,
          0.23934833705425262,
          0.23880872130393982,
          0.23828090727329254,
          0.2377653270959854,
          0.23726239800453186,
          0.23677249252796173,
          0.23629601299762726,
          0.2358333170413971,
          0.2353847324848175,
          0.23495060205459595,
          0.2345312386751175,
          0.23412691056728363,
          0.23373791575431824,
          0.23336447775363922,
          0.23300686478614807,
          0.2326652556657791,
          0.23233987390995026,
          0.23203088343143463,
          0.23173844814300537,
          0.23146268725395203,
          0.23120371997356415,
          0.2309616506099701,
          0.23073655366897583,
          0.23052845895290375,
          0.23033744096755981,
          0.23016346991062164,
          0.23000657558441162,
          0.22986671328544617,
          0.2297438532114029,
          0.22963790595531464,
          0.22954881191253662,
          0.2294764667749405,
          0.2294207364320755,
          0.2293814867734909,
          0.22935856878757477,
          0.22935180366039276,
          0.22936099767684937,
          0.22938595712184906,
          0.22942642867565155,
          0.22948220372200012,
          0.2295530140399933,
          0.22963859140872955,
          0.22973865270614624,
          0.22985289990901947,
          0.22998102009296417,
          0.23012268543243408,
          0.23027758300304413,
          0.23044534027576447,
          0.23062561452388763,
          0.23081804811954498,
          0.23102223873138428,
          0.23123781383037567,
          0.23146438598632812,
          0.23170152306556702,
          0.2319488525390625,
          0.23220594227313995,
          0.23247236013412476,
          0.23274768888950348,
          0.2330314815044403,
          0.23332332074642181,
          0.23362275958061218,
          0.2339293360710144,
          0.23424260318279266,
          0.23456214368343353,
          0.23488746583461761,
          0.2352181375026703,
          0.23555371165275574,
          0.23589371144771576,
          0.23623770475387573,
          0.23658522963523865,
          0.2369358390569687,
          0.23728907108306885,
          0.2376444935798645,
          0.23800165951251984,
          0.23836012184619904,
          0.23871944844722748,
          0.23907920718193054,
          0.2394389510154724,
          0.23979826271533966,
          0.24015673995018005,
          0.24051393568515778,
          0.2408694624900818,
          0.24122290313243866,
          0.24157387018203735,
          0.24192197620868683,
          0.24226681888103485,
          0.24260802567005157,
          0.24294523894786835,
          0.24327807128429413,
          0.24360619485378265,
          0.24392923712730408,
          0.24424687027931213,
          0.24455876648426056,
          0.2448645830154419,
          0.24516403675079346,
          0.2454567849636078,
          0.2457425594329834,
          0.24602104723453522,
          0.24629199504852295,
          0.2465551197528839,
          0.2468101531267166,
          0.24705685675144196,
          0.24729499220848083,
          0.24752430617809296,
          0.2477445900440216,
          0.24795562028884888,
          0.24815721809864044,
          0.2483491748571396,
          0.248531311750412,
          0.24870343506336212,
          0.24886542558670044,
          0.24901708960533142,
          0.24915829300880432,
          0.2492889165878296,
          0.24940884113311768,
          0.24951794743537903,
          0.2496161162853241,
          0.2497032880783081,
          0.2497793585062027,
          0.2498442828655243,
          0.24989797174930573,
          0.24994038045406342,
          0.24997149407863617,
          0.2499912679195404,
          0.24999967217445374,
          0.24999672174453735,
          0.24998240172863007,
          0.24995672702789307,
          0.24991974234580994,
          0.24987144768238068,
          0.24981191754341125,
          0.24974119663238525,
          0.24965934455394745,
          0.2495664358139038,
          0.2494625747203827,
          0.24934783577919006,
          0.24922232329845428,
          0.2490861862897873,
          0.24893951416015625,
          0.2487824708223343,
          0.24861519038677216,
          0.24843783676624298,
          0.2482505738735199,
          0.24805358052253723,
          0.2478470504283905,
          0.24763117730617523,
          0.24740616977214813,
          0.2471722513437271,
          0.24692963063716888,
          0.24667856097221375,
          0.246419295668602,
          0.24615207314491272,
          0.24587716162204742,
          0.2455948442220688,
          0.2453054040670395,
          0.2450091391801834,
          0.24470633268356323,
          0.24439731240272522,
          0.24408239126205444,
          0.24376191198825836,
          0.24343618750572205,
          0.24310556054115295,
          0.24277040362358093,
          0.24243107438087463,
          0.2420879304409027,
          0.241741344332695,
          0.24139170348644257,
          0.24103939533233643,
          0.24068482220172882,
          0.2403283715248108,
          0.23997046053409576,
          0.23961150646209717,
          0.23925191164016724,
          0.2388921082019806,
          0.23853252828121185,
          0.23817358911037445,
          0.2378157526254654,
          0.2374594360589981,
          0.2371050864458084,
          0.2367531657218933,
          0.23640410602092743,
          0.23605836927890778,
          0.23571638762950897,
          0.23537863790988922,
          0.23504555225372314,
          0.23471760749816895,
          0.23439523577690125,
          0.23407889902591705,
          0.23376904428005219,
          0.23346613347530365,
          0.23317059874534607,
          0.23288288712501526,
          0.23260343074798584,
          0.23233267664909363,
          0.23207105696201324,
          0.2318190038204193,
          0.23157691955566406,
          0.23134522140026093,
          0.23112431168556213,
          0.23091460764408112,
          0.2307164967060089,
          0.23053033649921417,
          0.23035652935504913,
          0.23019540309906006,
          0.230047345161438,
          0.2299126833677292,
          0.22979173064231873,
          0.22968482971191406,
          0.22959226369857788,
          0.22951433062553406,
          0.22945131361484528,
          0.22940346598625183,
          0.22937102615833282,
          0.22935424745082855,
          0.22935332357883453,
          0.22936846315860748,
          0.22939984500408173,
          0.2294476479291916,
          0.22951200604438782,
          0.22959305346012115,
          0.22969089448451996,
          0.2298056185245514,
          0.22993731498718262,
          0.23008602857589722,
          0.23025180399417877,
          0.23043464124202728,
          0.23063454031944275,
          0.23085148632526398,
          0.2310854196548462,
          0.23133628070354462,
          0.23160399496555328,
          0.23188844323158264,
          0.23218950629234314,
          0.23250703513622284,
          0.2328408807516098,
          0.23319083452224731,
          0.23355670273303986,
          0.23393826186656952,
          0.234335258603096,
          0.23474745452404022,
          0.2351745367050171,
          0.23561622202396393,
          0.23607218265533447,
          0.23654209077358246,
          0.23702558875083923,
          0.23752231895923615,
          0.23803187906742096,
          0.23855386674404144,
          0.23908786475658417,
          0.2396334558725357,
          0.24019017815589905,
          0.24075758457183838,
          0.2413351833820343,
          0.2419224977493286,
          0.24251903593540192,
          0.24312426149845123,
          0.24373769760131836,
          0.24435877799987793,
          0.24498698115348816,
          0.24562177062034607,
          0.2462625652551651,
          0.24690881371498108,
          0.24755996465682983,
          0.2482154220342636,
          0.24887461960315704,
          0.24953696131706238,
          0.25020188093185425,
          0.2508687973022461,
          0.2515370845794678,
          0.2522061765193939,
          0.2528754472732544,
          0.2535443603992462,
          0.25421231985092163,
          0.2548786699771881,
          0.25554290413856506,
          0.25620436668395996,
          0.2568625211715698,
          0.2575168013572693,
          0.2581665813922882,
          0.2588113248348236,
          0.2594504952430725,
          0.26008349657058716,
          0.26070982217788696,
          0.26132887601852417,
          0.2619401812553406,
          0.2625431716442108,
          0.2631373703479767,
          0.2637222409248352,
          0.2642973065376282,
          0.264862060546875,
          0.26541605591773987,
          0.2659588158130646,
          0.2664898931980133,
          0.26700881123542786,
          0.2675152122974396,
          0.26800861954689026,
          0.26848864555358887,
          0.26895490288734436,
          0.2694070041179657,
          0.26984459161758423,
          0.2702673375606537,
          0.2706748843193054,
          0.2710668742656708,
          0.27144306898117065,
          0.2718031108379364,
          0.2721467614173889,
          0.2724737524986267,
          0.2727837860584259,
          0.2730766832828522,
          0.27335217595100403,
          0.27361008524894714,
          0.2738502323627472,
          0.2740723788738251,
          0.2742764353752136,
          0.2744622230529785,
          0.2746295928955078,
          0.27477845549583435,
          0.2749086618423462,
          0.27502021193504333,
          0.27511292695999146,
          0.27518683671951294,
          0.2752418518066406,
          0.2752779722213745,
          0.2752951681613922,
          0.2752934396266937,
          0.27527281641960144,
          0.27523329854011536,
          0.27517497539520264,
          0.27509787678718567,
          0.2750020921230316,
          0.27488771080970764,
          0.2747548520565033,
          0.27460357546806335,
          0.27443405985832214,
          0.2742464542388916,
          0.2740408778190613,
          0.2738175690174103,
          0.27357664704322815,
          0.2733183205127716,
          0.27304285764694214,
          0.2727504372596741,
          0.2724412977695465,
          0.27211570739746094,
          0.27177393436431885,
          0.27141624689102173,
          0.27104291319847107,
          0.27065426111221313,
          0.2702506184577942,
          0.2698322832584381,
          0.26939961314201355,
          0.2689529061317444,
          0.26849260926246643,
          0.26801902055740356,
          0.2675325572490692,
          0.2670336067676544,
          0.2665225565433502,
          0.26599985361099243,
          0.2654658854007721,
          0.264921098947525,
          0.26436594128608704,
          0.26380085945129395,
          0.26322633028030396,
          0.2626428008079529,
          0.2620507776737213,
          0.26145070791244507,
          0.2608431279659271,
          0.2602285146713257,
          0.25960737466812134,
          0.25898024439811707,
          0.25834763050079346,
          0.2577100694179535,
          0.25706809759140015,
          0.2564222514629364,
          0.25577306747436523,
          0.255121111869812,
          0.2544669210910797,
          0.2538110911846161,
          0.25315412878990173,
          0.2524966597557068,
          0.25183919072151184,
          0.25118234753608704,
          0.25052666664123535,
          0.24987274408340454,
          0.24922113120555878,
          0.24857240915298462,
          0.24792715907096863,
          0.24728594720363617,
          0.2466493546962738,
          0.2460179328918457,
          0.24539227783679962,
          0.24477294087409973,
          0.2441604733467102,
          0.2435554563999176,
          0.2429584264755249,
          0.24236993491649628,
          0.2417905330657959,
          0.24122075736522675,
          0.2406611293554306,
          0.24011218547821045,
          0.23957443237304688,
          0.23904837667942047,
          0.23853451013565063,
          0.23803335428237915,
          0.23754535615444183,
          0.2370709925889969,
          0.23661071062088013,
          0.23616498708724976
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Model Output vs Target for Feature 3, Batch 2"
        },
        "xaxis": {
         "title": {
          "text": "Timestep"
         }
        },
        "yaxis": {
         "title": {
          "text": "Value"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_model_output_vs_target(model_outputs, targets, batch_index=0, feature_index=0):\n",
    "    # Extract the specified feature for the given batch from both the model outputs and targets\n",
    "    model_output_series = model_outputs[batch_index, :, feature_index].detach().numpy()\n",
    "    target_series = targets[batch_index, :, feature_index].numpy()\n",
    "    \n",
    "    # Create a range for the x-axis (timesteps)\n",
    "    timesteps = list(range(model_output_series.shape[0]))\n",
    "    \n",
    "    # Create traces\n",
    "    model_trace = go.Scatter(x=timesteps, y=model_output_series, mode='lines', name='Model Output')\n",
    "    target_trace = go.Scatter(x=timesteps, y=target_series, mode='lines', name='Target')\n",
    "    \n",
    "    # Create the figure and add traces\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(model_trace)\n",
    "    fig.add_trace(target_trace)\n",
    "    \n",
    "    # Add title and labels\n",
    "    fig.update_layout(title=f'Model Output vs Target for Feature {feature_index}, Batch {batch_index}',\n",
    "                      xaxis_title='Timestep',\n",
    "                      yaxis_title='Value')\n",
    "    \n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "# Assuming `y` and `Y` are your model outputs and targets, respectively\n",
    "# Adjust batch_index and feature_index as needed\n",
    "plot_model_output_vs_target(outputs, Y, batch_index=2, feature_index=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2491, 0.2306],\n",
       "        [0.2495, 0.2309],\n",
       "        [0.2500, 0.2312],\n",
       "        ...,\n",
       "        [0.2500, 0.2720],\n",
       "        [0.2496, 0.2719],\n",
       "        [0.2492, 0.2717]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([outputs[1,:,1],Y[1,:,1]], dim = -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coilspy-1sgZ1XBf-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
