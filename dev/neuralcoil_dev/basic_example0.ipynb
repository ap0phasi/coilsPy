{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        # Initialize the interaction tensor as a learnable parameter\n",
    "        self.interaction_tensor = nn.Parameter(torch.randn(n_features, n_features, n_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be of size [batch, length, n_features]\n",
    "        batch, length, n_features = x.size()\n",
    "\n",
    "        # Process each [n_features] vector across batch and length\n",
    "        output = x.new_empty(batch, length, n_features)\n",
    "        for b in range(batch):\n",
    "            for l in range(length):\n",
    "                state_tensor = x[b, l, :]  # Shape: [n_features]\n",
    "                # Step 1: Multiply state tensor by interaction tensor to get transition tensor\n",
    "                # We manually implement the multiplication to match your operation\n",
    "                transition_tensor = torch.einsum('i,ijk->jk', state_tensor, self.interaction_tensor)\n",
    "                # Step 2: Multiply the transition tensor by the state tensor\n",
    "                # Resulting shape: [n_features]\n",
    "                output[b, l, :] = torch.matmul(transition_tensor, state_tensor)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLayerVectorized(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(CustomLayerVectorized, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        # Initialize the interaction tensor as a learnable parameter\n",
    "        self.interaction_tensor = nn.Parameter(torch.randn(n_features, n_features, n_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be of size [batch, length, n_features]\n",
    "        batch, length, n_features = x.size()\n",
    "\n",
    "        # Pre-allocate output tensor\n",
    "        output = x.new_empty(batch, length, n_features)\n",
    "\n",
    "        # Loop over length, but vectorize over the batch\n",
    "        for l in range(length):\n",
    "            # Extract all vectors at position l across all batches\n",
    "            state_tensor = x[:, l, :]  # Shape: [batch, n_features]\n",
    "            \n",
    "            # Vectorized operation for all batches\n",
    "            # Step 1: Calculate the transition tensor\n",
    "            # Since we cannot directly use einsum for batched operation in this specific scenario,\n",
    "            # we manually broadcast and multiply to achieve the intended result.\n",
    "            # This involves expanding dimensions to enable broadcasting.\n",
    "            state_tensor_expanded = state_tensor.unsqueeze(1).expand(-1, n_features, -1)  # Shape: [batch, n_features, n_features]\n",
    "            interaction_tensor_expanded = self.interaction_tensor.unsqueeze(0).expand(batch, -1, -1, -1)  # Shape: [batch, n_features, n_features, n_features]\n",
    "            # Multiply and sum over the last dimension to get the transition tensor\n",
    "            transition_tensor = torch.einsum('bik,bijk->bij', state_tensor_expanded, interaction_tensor_expanded)\n",
    "            \n",
    "            # Step 2: Multiply the transition tensor by the state tensor to get the output\n",
    "            output[:, l, :] = torch.einsum('bij,bj->bi', transition_tensor, state_tensor)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 16])\n"
     ]
    }
   ],
   "source": [
    "n_features = 16\n",
    "batch, length, dim = 2, 64, n_features\n",
    "x = torch.randn(batch, length, dim)\n",
    "model = CustomLayerVectorized(\n",
    "    n_features = n_features\n",
    ")\n",
    "y = model(x)\n",
    "\n",
    "print(x.shape)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionModule(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(InteractionModule, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        # Initialize a set of interaction tensors, one for the state tensor and one for each column of the transition tensor\n",
    "        self.interaction_tensors = nn.ParameterList([nn.Parameter(torch.randn(n_features, n_features, n_features)) for _ in range(n_features + 1)])\n",
    "\n",
    "    def forward(self, state_tensor, previous_transition_tensor):\n",
    "        # Assuming previous_transition_tensors is a list of transition tensors from the previous step\n",
    "        candidates = []\n",
    "        for i in range(n_features + 1):\n",
    "            if i == 0:  # Interaction with the state tensor\n",
    "                current_tensor = state_tensor\n",
    "            else:  # Interaction with columns of the previous transition tensor\n",
    "                current_tensor = previous_transition_tensor[:, :, i - 1]\n",
    "\n",
    "            interaction_tensor = self.interaction_tensors[i]\n",
    "            current_tensor_expanded = current_tensor.unsqueeze(1).expand(-1, n_features, -1)  # Shape: [batch, n_features, n_features]\n",
    "            interaction_tensor_expanded = interaction_tensor.unsqueeze(0).expand(batch, -1, -1, -1)  # Shape: [batch, n_features, n_features, n_features]\n",
    "            # Multiply and sum over the last dimension to get the transition tensor\n",
    "            candidate = torch.einsum('bik,bijk->bij', current_tensor_expanded, interaction_tensor_expanded)\n",
    "            \n",
    "            candidates.append(candidate)\n",
    "        return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectorModule(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(SelectorModule, self).__init__()\n",
    "        # Placeholder for any parameters or initialization\n",
    "        pass\n",
    "\n",
    "    def forward(self, candidates):\n",
    "        # Select one of the candidates based on a criterion\n",
    "        # This is a placeholder for the selection logic\n",
    "        # For demonstration, we just return the first candidate\n",
    "        return candidates[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayerExtended(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(CustomLayerExtended, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.interaction_module = InteractionModule(n_features)\n",
    "        self.selector_module = SelectorModule(n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, length, n_features = x.size()\n",
    "        output = x.new_empty(batch, length, n_features)\n",
    "\n",
    "        # Initialize previous transition tensors (for the first step)\n",
    "        # Assuming it's a list of zero tensors for simplicity\n",
    "        previous_transition_tensor = torch.zeros(batch, n_features, n_features)\n",
    "\n",
    "        for l in range(length):\n",
    "            state_tensor = x[:, l, :]\n",
    "            # Generate candidates\n",
    "            candidates = self.interaction_module(state_tensor, previous_transition_tensor)\n",
    "            # Select one candidate\n",
    "            selected_transition_tensor = self.selector_module(candidates)\n",
    "            # Update the previous_transition_tensors for the next iteration\n",
    "            previous_transition_tensor = selected_transition_tensor\n",
    "            # Compute output for this step\n",
    "            output[:, l, :] = torch.matmul(selected_transition_tensor, state_tensor.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        return output, selected_transition_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 16])\n"
     ]
    }
   ],
   "source": [
    "n_features = 16\n",
    "batch, length, dim = 2, 64, n_features\n",
    "x = torch.randn(batch, length, dim)\n",
    "model = CustomLayerExtended(\n",
    "    n_features = n_features\n",
    ")\n",
    "y = model(x)\n",
    "\n",
    "print(y[0].shape)\n",
    "assert y[0].shape == x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coilspy-1sgZ1XBf-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
