{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        # Initialize the interaction tensor as a learnable parameter\n",
    "        self.interaction_tensor = nn.Parameter(torch.randn(n_features, n_features, n_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be of size [batch, length, n_features]\n",
    "        batch, length, n_features = x.size()\n",
    "\n",
    "        # Process each [n_features] vector across batch and length\n",
    "        output = x.new_empty(batch, length, n_features)\n",
    "        for b in range(batch):\n",
    "            for l in range(length):\n",
    "                state_tensor = x[b, l, :]  # Shape: [n_features]\n",
    "                # Step 1: Multiply state tensor by interaction tensor to get transition tensor\n",
    "                # We manually implement the multiplication to match your operation\n",
    "                transition_tensor = torch.einsum('i,ijk->jk', state_tensor, self.interaction_tensor)\n",
    "                # Step 2: Multiply the transition tensor by the state tensor\n",
    "                # Resulting shape: [n_features]\n",
    "                output[b, l, :] = torch.matmul(transition_tensor, state_tensor)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomLayerVectorized(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(CustomLayerVectorized, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        # Initialize the interaction tensor as a learnable parameter\n",
    "        self.interaction_tensor = nn.Parameter(torch.randn(n_features, n_features, n_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected to be of size [batch, length, n_features]\n",
    "        batch, length, n_features = x.size()\n",
    "\n",
    "        # Pre-allocate output tensor\n",
    "        output = x.new_empty(batch, length, n_features)\n",
    "\n",
    "        # Loop over length, but vectorize over the batch\n",
    "        for l in range(length):\n",
    "            # Extract all vectors at position l across all batches\n",
    "            state_tensor = x[:, l, :]  # Shape: [batch, n_features]\n",
    "            \n",
    "            # Vectorized operation for all batches\n",
    "            # Step 1: Calculate the transition tensor\n",
    "            # Since we cannot directly use einsum for batched operation in this specific scenario,\n",
    "            # we manually broadcast and multiply to achieve the intended result.\n",
    "            # This involves expanding dimensions to enable broadcasting.\n",
    "            state_tensor_expanded = state_tensor.unsqueeze(1).expand(-1, n_features, -1)  # Shape: [batch, n_features, n_features]\n",
    "            interaction_tensor_expanded = self.interaction_tensor.unsqueeze(0).expand(batch, -1, -1, -1)  # Shape: [batch, n_features, n_features, n_features]\n",
    "            # Multiply and sum over the last dimension to get the transition tensor\n",
    "            transition_tensor = torch.einsum('bik,bijk->bij', state_tensor_expanded, interaction_tensor_expanded)\n",
    "            \n",
    "            # Step 2: Multiply the transition tensor by the state tensor to get the output\n",
    "            output[:, l, :] = torch.einsum('bij,bj->bi', transition_tensor, state_tensor)\n",
    "\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features = 16\n",
    "# batch, length, dim = 2, 64, n_features\n",
    "# x = torch.randn(batch, length, dim)\n",
    "# model = CustomLayerVectorized(\n",
    "#     n_features = n_features\n",
    "# )\n",
    "# y = model(x)\n",
    "\n",
    "# print(x.shape)\n",
    "# assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionModule(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(InteractionModule, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        # Initialize a set of interaction tensors, one for the state tensor and one for each column of the transition tensor\n",
    "        self.interaction_tensors = nn.ParameterList([nn.Parameter(torch.randn(n_features, n_features, n_features)) for _ in range(n_features + 1)])\n",
    "\n",
    "    def forward(self, state_tensor, previous_transition_tensor):\n",
    "        # Get batch size\n",
    "        batch = state_tensor.shape[0]\n",
    "        # Assuming previous_transition_tensors is a list of transition tensors from the previous step\n",
    "        candidates = []\n",
    "        for i in range(self.n_features + 1):\n",
    "            if i == 0:  # Interaction with the state tensor\n",
    "                current_tensor = state_tensor\n",
    "            else:  # Interaction with columns of the previous transition tensor\n",
    "                current_tensor = previous_transition_tensor[:, :, i - 1]\n",
    "\n",
    "            interaction_tensor = self.interaction_tensors[i]\n",
    "            current_tensor_expanded = current_tensor.unsqueeze(1).expand(-1, self.n_features, -1)  # Shape: [batch, n_features, n_features]\n",
    "            interaction_tensor_expanded = interaction_tensor.unsqueeze(0).expand(batch, -1, -1, -1)  # Shape: [batch, n_features, n_features, n_features]\n",
    "            # Multiply and sum over the last dimension to get the transition tensor\n",
    "            candidate = torch.einsum('bik,bijk->bij', current_tensor_expanded, interaction_tensor_expanded)\n",
    "            \n",
    "            candidates.append(candidate)\n",
    "            \n",
    "        candidates_tensor = torch.stack(candidates, dim = -1)\n",
    "        return candidates_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SelectorModule(nn.Module):\n",
    "    def __init__(self, num_slices):\n",
    "        super(SelectorModule, self).__init__()\n",
    "        # A simple linear layer to compute importance scores for each slice\n",
    "        self.importance = nn.Linear(num_slices, num_slices)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Assuming x is of shape [batch_size, height, width, num_slices]\n",
    "        batch_size, height, width, num_slices = x.shape\n",
    "\n",
    "        # Flatten the spatial dimensions and compute importance scores\n",
    "        x_flat = x.view(batch_size, -1, num_slices)  # New shape: [batch_size, height*width, num_slices]\n",
    "        scores = self.importance(x_flat)  # Computes a score for each slice\n",
    "        scores = scores.view(batch_size, height, width, num_slices)  # Reshape scores back\n",
    "\n",
    "        # Apply softmax to get a distribution over slices\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Use the weights to get a weighted sum of the slices, effectively selecting slices based on importance\n",
    "        # This step aggregates the slices into a single output per position\n",
    "        selected = torch.einsum('bhwn,bhwi->bhw', weights, x)\n",
    "\n",
    "        return selected\n",
    "\n",
    "# Example usage\n",
    "batch_size = 10\n",
    "tensor = torch.rand(batch_size, 5, 5, 6)  # Example tensor\n",
    "model = SelectorModule(num_slices=6)\n",
    "\n",
    "result = model(tensor)\n",
    "print(result.shape)  # Should print torch.Size([10, 5, 5]), indicating the selection/aggregation step was performed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayerExtended(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super(CustomLayerExtended, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.interaction_module = InteractionModule(n_features)\n",
    "        self.selector_module = SelectorModule(n_features + 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, length, n_features = x.size()\n",
    "        output = x.new_empty(batch, length, n_features)\n",
    "\n",
    "        # Initialize previous transition tensors (for the first step)\n",
    "        # Assuming it's a list of zero tensors for simplicity\n",
    "        previous_transition_tensor = torch.zeros(batch, n_features, n_features)\n",
    "\n",
    "        for l in range(length):\n",
    "            state_tensor = x[:, l, :]\n",
    "            # Generate candidates\n",
    "            candidates = self.interaction_module(state_tensor, previous_transition_tensor)\n",
    "            # Select one candidate\n",
    "            selected_transition_tensor = self.selector_module(candidates)\n",
    "            # Update the previous_transition_tensors for the next iteration\n",
    "            previous_transition_tensor = selected_transition_tensor\n",
    "            # Compute output for this step\n",
    "            output[:, l, :] = torch.matmul(selected_transition_tensor, state_tensor.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        return output, selected_transition_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 64, 16])\n"
     ]
    }
   ],
   "source": [
    "n_features = 16\n",
    "batch, length, dim = 13, 64, n_features\n",
    "x = torch.randn(batch, length, dim)\n",
    "model = CustomLayerExtended(\n",
    "    n_features = n_features\n",
    ")\n",
    "y = model(x)\n",
    "\n",
    "print(y[0].shape)\n",
    "assert y[0].shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 1.3828e+01,  7.3758e+00, -2.3138e+01,  ..., -2.0552e+01,\n",
      "           1.2001e+01, -3.8707e+01],\n",
      "         [ 1.1377e+01, -1.4783e+01, -1.3818e+02,  ..., -1.4889e+02,\n",
      "          -2.9613e+01,  1.0292e+02],\n",
      "         [-4.6556e+03, -1.4859e+04,  5.3426e+03,  ...,  4.6355e+03,\n",
      "          -6.7901e+03, -4.6900e+03],\n",
      "         ...,\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan]],\n",
      "\n",
      "        [[ 3.2364e+01, -2.3503e+01, -1.8020e+01,  ..., -2.9927e+00,\n",
      "          -1.3689e+01,  2.3359e+01],\n",
      "         [ 6.8940e+01,  2.3408e+02,  2.2757e+02,  ...,  4.1637e+01,\n",
      "           6.7449e+01, -2.5223e+02],\n",
      "         [-2.2979e+03, -2.1993e+03,  3.0195e+03,  ...,  9.1770e+03,\n",
      "          -3.3622e+03, -8.3801e+02],\n",
      "         ...,\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan]],\n",
      "\n",
      "        [[-1.5340e+01, -2.2160e-01, -3.2563e+01,  ..., -2.5485e+01,\n",
      "          -2.1241e+01,  1.9013e+01],\n",
      "         [ 1.8641e+02,  4.1433e+02,  7.3702e+01,  ..., -6.5441e+02,\n",
      "           4.0423e+02, -5.0495e+02],\n",
      "         [ 7.4394e+03,  4.2155e+03,  3.5252e+03,  ..., -6.8858e+03,\n",
      "           7.2499e+03, -4.9146e+03],\n",
      "         ...,\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.9178e+01,  2.3544e+01, -1.9920e+01,  ..., -1.1969e+00,\n",
      "           1.2489e+01,  9.8268e+00],\n",
      "         [ 3.4103e-01, -1.8441e+02,  7.8794e+01,  ...,  6.8809e+01,\n",
      "           2.7943e+02, -1.0481e+02],\n",
      "         [ 1.1077e+03,  2.2092e+03, -7.9184e+03,  ..., -2.7785e+03,\n",
      "          -8.2454e+03,  2.3908e+03],\n",
      "         ...,\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan]],\n",
      "\n",
      "        [[-4.1402e+00,  8.5291e+00,  1.1280e+01,  ..., -4.1566e+00,\n",
      "           9.4080e+00, -2.4804e+00],\n",
      "         [ 3.7731e+01, -5.6415e+01, -1.4254e+02,  ...,  1.6450e+02,\n",
      "           6.9289e+01,  1.5210e+02],\n",
      "         [-1.2143e+03, -6.6137e+03,  6.4562e+03,  ..., -1.9210e+02,\n",
      "          -2.8776e+03, -2.2788e+03],\n",
      "         ...,\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan]],\n",
      "\n",
      "        [[-2.5444e+01, -1.4436e+01,  1.5880e+01,  ..., -1.3085e+01,\n",
      "           1.5286e+01, -8.7371e+00],\n",
      "         [-1.7063e+02,  2.0961e+02,  2.6220e+02,  ...,  1.0251e+02,\n",
      "           4.1934e+02,  1.2439e+02],\n",
      "         [-9.3100e+02,  2.8444e+03,  1.2969e+03,  ...,  2.2770e+03,\n",
      "          -5.4185e+02, -1.5164e+03],\n",
      "         ...,\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan],\n",
      "         [        nan,         nan,         nan,  ...,         nan,\n",
      "                  nan,         nan]]], grad_fn=<CopySlices>), tensor([[[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]],\n",
      "\n",
      "        [[nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         ...,\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan],\n",
      "         [nan, nan, nan,  ..., nan, nan, nan]]], grad_fn=<ViewBackward0>))\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def generate_multivariate_time_series(n_features, total_length, amplitude=1.0):\n",
    "    t = np.linspace(0, 4 * np.pi, total_length)\n",
    "    series = np.zeros((total_length, n_features))\n",
    "    for i in range(n_features):\n",
    "        series[:, i] = amplitude * np.cos(t * (i + 1) / n_features)\n",
    "    return series\n",
    "\n",
    "def segment_time_series(series, length):\n",
    "    # Assuming series is a numpy array of shape [total_length, n_features]\n",
    "    total_length, n_features = series.shape\n",
    "    segments = []\n",
    "    for start in range(0, total_length - length, length):\n",
    "        segment = series[start:start + length]\n",
    "        segments.append(segment)\n",
    "    return np.stack(segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 12\n",
    "length = 64  # Segment length\n",
    "total_length = 1024  # Arbitrary total length for the generated series\n",
    "\n",
    "# Generate and segment the time series\n",
    "series = generate_multivariate_time_series(n_features, total_length)\n",
    "segments = segment_time_series(series, length)\n",
    "\n",
    "# Convert to tensors\n",
    "segments_tensor = torch.tensor(segments, dtype=torch.float)\n",
    "\n",
    "# Prepare inputs and targets\n",
    "X = segments_tensor\n",
    "# Shift segments to the right by one timestep to create the targets\n",
    "Y = torch.cat((X[:, 1:], X[:, :1]), dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: nan\n",
      "Epoch [20/100], Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, Y)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\johnm\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\coilspy-1sgZ1XBf-py3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\johnm\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\coilspy-1sgZ1XBf-py3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 19\u001b[0m, in \u001b[0;36mCustomLayerExtended.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m x[:, l, :]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Generate candidates\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteraction_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_transition_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Select one candidate\u001b[39;00m\n\u001b[0;32m     21\u001b[0m selected_transition_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselector_module(candidates)\n",
      "File \u001b[1;32mc:\\Users\\johnm\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\coilspy-1sgZ1XBf-py3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\johnm\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\coilspy-1sgZ1XBf-py3.12\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 23\u001b[0m, in \u001b[0;36mInteractionModule.forward\u001b[1;34m(self, state_tensor, previous_transition_tensor)\u001b[0m\n\u001b[0;32m     21\u001b[0m     interaction_tensor_expanded \u001b[38;5;241m=\u001b[39m interaction_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(batch, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: [batch, n_features, n_features, n_features]\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Multiply and sum over the last dimension to get the transition tensor\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbik,bijk->bij\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_tensor_expanded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minteraction_tensor_expanded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     candidates\u001b[38;5;241m.\u001b[39mappend(candidate)\n\u001b[0;32m     27\u001b[0m candidates_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(candidates, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\johnm\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\coilspy-1sgZ1XBf-py3.12\\Lib\\site-packages\\torch\\functional.py:380\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    382\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "# Model\n",
    "model = CustomLayerExtended(\n",
    "    n_features = n_features\n",
    ")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    outputs, _ = model(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "\n",
    "    # Backward and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coilspy-1sgZ1XBf-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
